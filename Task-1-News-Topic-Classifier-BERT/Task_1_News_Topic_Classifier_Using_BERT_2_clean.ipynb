{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTllD3lqkHG-"
   },
   "source": [
    "# Task 1: News Topic Classifier Using BERT (AG News)\n",
    "**Objective:** Fine-tune `bert-base-uncased` to classify news headlines into 4 topics (AG News).\n",
    "\n",
    "**What this notebook does (high-level)**\n",
    "1. Install required libraries and check GPU.\n",
    "2. Load AG News dataset from Hugging Face `datasets`.\n",
    "3. Tokenize and preprocess (BERT tokenizer).\n",
    "4. Fine-tune `bert-base-uncased` using Hugging Face `Trainer`.\n",
    "5. Evaluate (accuracy, F1).\n",
    "6. Create a simple **Gradio** app to try the model live.\n",
    "\n",
    "**Notes for beginners**\n",
    "- This notebook uses the `datasets` and `transformers` libraries which handle dataset fetching and training.\n",
    "- If you see memory issues during training, reduce `per_device_train_batch_size` (e.g., to 8).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14594,
     "status": "ok",
     "timestamp": 1756013767986,
     "user": {
      "displayName": "Muhammad Tahir",
      "userId": "03468324879887043861"
     },
     "user_tz": -300
    },
    "id": "4_n7pU-mkGS3",
    "outputId": "d1bcf809-43bd-4355-b9f2-6cea64990200"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install -q transformers datasets evaluate accelerate gradio scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12912,
     "status": "ok",
     "timestamp": 1756013783473,
     "user": {
      "displayName": "Muhammad Tahir",
      "userId": "03468324879887043861"
     },
     "user_tz": -300
    },
    "id": "6hTmlGqRkMtp",
    "outputId": "40822f02-4f2d-4874-d2bc-c480c2c3148d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.8.0+cu126\n",
      "Transformers version: 4.55.2\n",
      "Datasets version: 4.0.0\n",
      "CUDA available: True\n",
      "GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability and versions\n",
    "import torch, transformers, datasets\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"Transformers version:\", transformers.__version__)\n",
    "print(\"Datasets version:\", datasets.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWUWysCUlfJf"
   },
   "source": [
    "## 1) Load the AG News dataset\n",
    "We will use the `ag_news` built into HF `datasets`. It has 4 classes:\n",
    "0 → World\n",
    "1 → Sports\n",
    "2 → Business\n",
    "3 → Sci/Tech\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475,
     "referenced_widgets": [
      "199e2a7abeb445b3a1cf6d08e9ed343a",
      "e768f41981154e2980ba14c201002b47",
      "44ff3f4970924026b48ca5b5682ec444",
      "7e24df93652948d488936d6eb90fc585",
      "4a362a7d6df4497caf767c6d26dde21e",
      "445bd21c33024b60bf06caa88d8f0544",
      "58694a6f6e2f48699d58fd72b29fb831",
      "ecab340e44fc481fbe3a35e9b78b6078",
      "43d86786fd8947f8aad7e08db62f70a8",
      "b848c864ed204854ab77a3ab2f1f7ccb",
      "4b3257c533ef4d1d96a96f3e2ebcb400",
      "1505b6abbdd54a21b8101b390b26d9f3",
      "f09f73d61ba2461e90f5721d542ca046",
      "4b5706d2baf14a88a70c0c77bbde8f78",
      "6cdfe76175354f4cbed493a7023acc10",
      "2f4f311acc72428cb6b30a2f4e7f0f49",
      "c25b9324f1ed4b5da3750746410882c6",
      "57544b7e95144d3fae42957f5aec5398",
      "7a3ddff8cf054e8daae8c01df8d0a3f9",
      "5245313d45bb41b3833dd47e70432e9d",
      "05b377412b7b4e25b7cbe16a38858baa",
      "6dd124df4ac5444fb80f75aae2b742a3",
      "2a465fc9129d4efda6d887cbe1d42d5a",
      "2c23ba1a8ebe4157bbb367c8dc9d95e6",
      "ddfca5da1f3b42f78a15f1fb8a59e3dc",
      "6d764ab69f0b497283abf29f3de28475",
      "bf3fe51e71714082a9c3327228e55bae",
      "8dc6962f010943ba8da4851540069b67",
      "9a13b05c380349bb8b43ed956c8f49c7",
      "cb4cf1aa7fd24be4b51652d6943bf0c4",
      "14bda6546d8e4adba1f9c11f61108ba3",
      "2a92878d14654d7e9c577e009d274e46",
      "bc3cc0a79b90400f813104268146aa22",
      "409e3788ca3b48b79ed55065ed518090",
      "bf67ba57046746d1a51c491785284920",
      "f1b8cbed289044f9b7be4f3a166b3944",
      "34c9ae0c2e704bb0bae1231611dcbdbc",
      "13bead2115d74e40b4238c05159b4255",
      "783e7dfe337349cb887dd3e2a8d55dbe",
      "abf07c075b9348c3ab45f8d9133d3279",
      "f740ea00e90c4dac9c090974e7be9fbc",
      "ed438512794441b4a8a0d7e31d3838e1",
      "04147b981fd848cf9572d0d4deeb5b62",
      "606f0bed500b46c8963dede679507feb",
      "202f6e4beabf4675a0b5243d4a7a71f9",
      "9af151c0f27e4237b4f8b5e61d681a7f",
      "91886dba9208462788cd9986ac553337",
      "af81024cc7c348829283792478383475",
      "ef62fa180e984e4b84548fc0da24657d",
      "66abf6ff11c14843898422c19e4fca7d",
      "b6fa3a69a42c4b5ca0c3d5b10816eab3",
      "199c13a566294f57945188adfb97ba0b",
      "5678c6d52666442097430cf61bcedd82",
      "027e95e41ef74b3cae57aea201636162",
      "a148077094c64535b08257a67a670bd9"
     ]
    },
    "executionInfo": {
     "elapsed": 9699,
     "status": "ok",
     "timestamp": 1756013937607,
     "user": {
      "displayName": "Muhammad Tahir",
      "userId": "03468324879887043861"
     },
     "user_tz": -300
    },
    "id": "LnrS_FfikZnq",
    "outputId": "24b68510-678c-4cd0-e200-64bc154efefe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/18.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 120000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 7600\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load AG News\n",
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"ag_news\")\n",
    "raw_datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-loqWRplm-K"
   },
   "source": [
    "## 2) Prepare tokenizer & tokenize dataset\n",
    "We will use `bert-base-uncased` tokenizer with `max_length=128`. We will tokenize the **text** (headline) and keep labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455,
     "referenced_widgets": [
      "95bf440dda4f4c81bca031ca6d1bf7c7",
      "ef106abb6adf46b9bcae4088fe5d40e4",
      "6ee108126d0046feafe2662e12ec3e76",
      "b17500523ce04dd185f89f658f5fc1a0",
      "84bb47ee24a84b20881c1ce6b3fc5fb6",
      "f5ad963bdbaa4d9199c27ddf63e6d6df",
      "c4fd677334bd4f65a000dfea87bc6c04",
      "37f993923b3e4d789dd41bb37322d395",
      "4500091e418e48e0aa9fbf511b3966f1",
      "9dbe72372fed415482cf2c8980b0dd33",
      "cef459633dae4fd3a3b860fb10338584",
      "72497f2626554246bd2506f76ba94a61",
      "1644d1b57b66431b80209ce9ab68e0c4",
      "b9d48cd7af734c5aba4f75019cee40d6",
      "bec820c400214e0c99e8144cb584d8b3",
      "513db679ec5d45228a07e52bcb38bf1c",
      "248e51fdc1dc4e75bb053be6b101ebc6",
      "b8a476e6ca804871ad7f0ac5f33dc6c5",
      "83dc78c23ae748ef8eb92c5a577ede1d",
      "8cc717aed35c4cde97df7b29998def85",
      "d363c1c6a6b54caf802c522be1931b33",
      "a79fa104a0d04ed8ade4636fdd30a6a1",
      "8e5ac890d4fb48c98dd2e6e40982cbe6",
      "063bf73259674ebe85200a44eee71d5f",
      "9d059a6710d34dcbad49224ac0e80e62",
      "09888fe0c3fc485d90e05a801b067e26",
      "812911ff03014fe39f18894532cd04a1",
      "94f55d0b7e004ab283ad1a81826054f7",
      "752914588ade40cd90146db26d13b6ff",
      "d9bf07bd5229435ca7c2acec35851349",
      "c0ed418167344d46bb01a515becbd89c",
      "8dc6a6edbcaa4490b5c6143d5fc638d6",
      "95aa3cd397c84e8cafb931b56867f5e3",
      "b607867dcf5340109e3a0e0bb408daa5",
      "8544bc294eb24b1f849c72a4770e2add",
      "9b85b17d4bbe4be1b1b378ce5eb36a0d",
      "2fc7bf729266481cb22260a156b4a63b",
      "6dba98b73f4548eda9a2d257ff629c0e",
      "34083749489f4aa7b0424581ed092963",
      "300812fa7ea24a3ea21aba93363a890c",
      "14f4dc1fde4345328cc70c1487e9500b",
      "4a99046de56f4b22982307dac5c11a48",
      "8db3dc857e1a41d3be85fd9098c853d9",
      "ff9ff1ccc1ad4f8199c9714f18b20e82",
      "14606991cdd740e29fa0882062448873",
      "08636b52ad87457081bdad55f917a308",
      "c43a14e2cad841f0875e23604783a3c1",
      "71d2d5addd054cbd8cd6e5dc6eb87019",
      "02d91684bfd24d6a85268c3a18427dd9",
      "4134f461755d4751b00f29e19157e36b",
      "dee583ed22fb4bd4b06530199017f7ee",
      "168069f45e564a90a8e3f01dfcc8af2d",
      "e97823d98ef94ca9bacd6d37a0dafdf0",
      "7aa2371e413040cc9da85d5bf2c71b27",
      "9d79432ac3c54f90ab96292d6a19cf41",
      "e16e72ac1053498692e298fc377f97d2",
      "2dddc381e20c4efabadaa3a36c30ff0b",
      "f8c28b7c968f47c2b63533d20deffec6",
      "dd4f81dea3a949edb47a2374c29f5fba",
      "7344868fd37049f88c5e23b866e317f8",
      "678686e4de6a434f90994d6a0e11e8a7",
      "c0ca92cfc6374f20b0e1cfecc6618f57",
      "a5d12b7d9c7a41c0884d6406071717c0",
      "059f2df21b2d4a89af802d145df08bb1",
      "f5afee7346674bd8bdf3f56b9cde8e6c",
      "b705f5a1f27a449287931b38c07a7132"
     ]
    },
    "executionInfo": {
     "elapsed": 42098,
     "status": "ok",
     "timestamp": 1756015295933,
     "user": {
      "displayName": "Muhammad Tahir",
      "userId": "03468324879887043861"
     },
     "user_tz": -300
    },
    "id": "C2-tmjmSlhrX",
    "outputId": "5b8f922f-7143-48af-b7b9-f811a861a293"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizerFast'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/7600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 120000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 7600\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize dataset\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "max_length = 128\n",
    "\n",
    "def preprocess(batch):\n",
    "    # AG News uses column 'text' for headline+description; we'll use it as input\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(preprocess, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "executionInfo": {
     "elapsed": 14864,
     "status": "ok",
     "timestamp": 1756015314163,
     "user": {
      "displayName": "Muhammad Tahir",
      "userId": "03468324879887043861"
     },
     "user_tz": -300
    },
    "id": "7eEgDWmuqt8G",
    "outputId": "ece6e44f-0ddf-4995-c0c7-166cc7bb3313"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       World       0.25      0.87      0.38      1900\n",
      "      Sports       0.09      0.01      0.01      1900\n",
      "    Business       0.28      0.01      0.02      1900\n",
      "    Sci/Tech       0.16      0.06      0.08      1900\n",
      "\n",
      "    accuracy                           0.24      7600\n",
      "   macro avg       0.19      0.24      0.13      7600\n",
      "weighted avg       0.19      0.24      0.13      7600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "preds_output = trainer.predict(tokenized_datasets[\"test\"])\n",
    "y_true = preds_output.label_ids\n",
    "y_pred = preds_output.predictions.argmax(axis=1)\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17i7MunHlsUS"
   },
   "source": [
    "## 3) Create train/validation splits\n",
    "Hugging Face `ag_news` already has `train` and `test`. We'll take a small validation split from train to evaluate during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 547,
     "status": "ok",
     "timestamp": 1756015319724,
     "user": {
      "displayName": "Muhammad Tahir",
      "userId": "03468324879887043861"
     },
     "user_tz": -300
    },
    "id": "ltQwTMaVlo5d",
    "outputId": "b88ef41b-22ed-42b4-93d5-ee27b575cb55"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 108000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 12000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 7600\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create train/validation/test splits\n",
    "from datasets import DatasetDict\n",
    "\n",
    "train_val = tokenized_datasets[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "ds = DatasetDict({\n",
    "    \"train\": train_val[\"train\"],\n",
    "    \"validation\": train_val[\"test\"],\n",
    "    \"test\": tokenized_datasets[\"test\"]\n",
    "})\n",
    "ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BS6G6Mqml-sd"
   },
   "source": [
    "## 4) Define model, training args, and metrics\n",
    "We'll use `AutoModelForSequenceClassification` with 4 labels and Trainer API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "79135fa23bcb4d6e91d9fe5fc3e418f7",
      "5d05933587e94924822ea2b030a176bf",
      "137f8f60cb3a42ca96f9b802ca62c33e",
      "6127fd9056174c57aa896caaaaaf5cef",
      "4d362eb325c24cd7ae1ef33f816d5f4d",
      "6a0bfdda5b0b4c15aadb7304ec0296fb",
      "5c5a3a04e52e4775a7739df7567c3aca",
      "b040117681204274bbd5ca2bc5e8057d",
      "44fef0661749408eb1d07880bcf11545",
      "a2a02e738e364c489a8db21921513c1e",
      "27756a8252b442b3831e8c0bd9267832"
     ]
    },
    "executionInfo": {
     "elapsed": 5937,
     "status": "ok",
     "timestamp": 1756015397511,
     "user": {
      "displayName": "Muhammad Tahir",
      "userId": "03468324879887043861"
     },
     "user_tz": -300
    },
    "id": "AkpwlPeDl8ea",
    "outputId": "a19c6c38-1642-4015-a369-059c653f1aca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Setup model and compute_metrics\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "num_labels = 4\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    prec = precision_score(labels, preds, average=\"weighted\")\n",
    "    rec = recall_score(labels, preds, average=\"weighted\")\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": prec, \"recall\": rec}\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_GBrmlBmH8V"
   },
   "source": [
    "### Training configuration\n",
    "- Keep epochs small initially (2) to finish quickly on Colab.\n",
    "- If you have time, increase `num_train_epochs` to 3–4 for better performance.\n",
    "- Reduce batch size if you run out of memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "executionInfo": {
     "elapsed": 2026303,
     "status": "ok",
     "timestamp": 1756017871552,
     "user": {
      "displayName": "Muhammad Tahir",
      "userId": "03468324879887043861"
     },
     "user_tz": -300
    },
    "id": "sCx0y7u7mBEn",
    "outputId": "f9cc4192-beab-4d0c-d155-281a89c8a404"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-1686622951.py:18: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30000' max='30000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30000/30000 33:35, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.175400</td>\n",
       "      <td>0.177009</td>\n",
       "      <td>0.941842</td>\n",
       "      <td>0.941910</td>\n",
       "      <td>0.942005</td>\n",
       "      <td>0.941842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.131200</td>\n",
       "      <td>0.180551</td>\n",
       "      <td>0.946842</td>\n",
       "      <td>0.946912</td>\n",
       "      <td>0.947109</td>\n",
       "      <td>0.946842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.085500</td>\n",
       "      <td>0.222765</td>\n",
       "      <td>0.943947</td>\n",
       "      <td>0.943970</td>\n",
       "      <td>0.944246</td>\n",
       "      <td>0.943947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.052600</td>\n",
       "      <td>0.259935</td>\n",
       "      <td>0.944868</td>\n",
       "      <td>0.944901</td>\n",
       "      <td>0.944982</td>\n",
       "      <td>0.944868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('news-bert-model/tokenizer_config.json',\n",
       " 'news-bert-model/special_tokens_map.json',\n",
       " 'news-bert-model/vocab.txt',\n",
       " 'news-bert-model/added_tokens.json',\n",
       " 'news-bert-model/tokenizer.json')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Define training arguments (for transformers >= 4.55)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",         # replaced evaluation_strategy\n",
    "    save_strategy=\"epoch\",         # keep saving per epoch\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\"               # disables wandb etc.\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# ✅ Save both model and tokenizer together\n",
    "trainer.save_model(\"news-bert-model\")\n",
    "tokenizer.save_pretrained(\"news-bert-model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z6M8NWEJmvOB"
   },
   "source": [
    "## 5) Evaluate on Test set\n",
    "We will run evaluation on the held-out test set and print metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "executionInfo": {
     "elapsed": 9586,
     "status": "ok",
     "timestamp": 1756018290251,
     "user": {
      "displayName": "Muhammad Tahir",
      "userId": "03468324879887043861"
     },
     "user_tz": -300
    },
    "id": "pkxkHHwwmNmM",
    "outputId": "c08f6f15-3816-4333-be3b-fbe03a42ed41"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='475' max='475' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [475/475 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25993505120277405, 'eval_accuracy': 0.9448684210526316, 'eval_f1': 0.9449006135528849, 'eval_precision': 0.9449821157258761, 'eval_recall': 0.9448684210526316, 'eval_runtime': 8.561, 'eval_samples_per_second': 887.75, 'eval_steps_per_second': 55.484, 'epoch': 4.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "metrics = trainer.evaluate(ds[\"test\"])\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "executionInfo": {
     "elapsed": 4138,
     "status": "ok",
     "timestamp": 1756018185952,
     "user": {
      "displayName": "Muhammad Tahir",
      "userId": "03468324879887043861"
     },
     "user_tz": -300
    },
    "id": "fg0q9xxAtKAJ",
    "outputId": "ba053d0f-d144-4a22-a830-16243ce50843"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DistilBertTokenizerFast'.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://2eb6d031c435b99282.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://2eb6d031c435b99282.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================\n",
    "# Deploying the model with Gradio\n",
    "# ============================\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, pipeline\n",
    "\n",
    "# Define label mapping (AG News has 4 classes)\n",
    "id2label = {\n",
    "    0: \"World\",\n",
    "    1: \"Sports\",\n",
    "    2: \"Business\",\n",
    "    3: \"Sci/Tech\"\n",
    "}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "# Load trained model + tokenizer\n",
    "model_path = \"news-bert-model\"\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_path)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    model_path,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# Create pipeline\n",
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Prediction function\n",
    "def predict_news_topic(text):\n",
    "    preds = classifier(text, truncation=True, max_length=512)\n",
    "    label = preds[0]['label']\n",
    "    score = round(preds[0]['score'], 3)\n",
    "    return f\"Predicted Topic: {label} (confidence: {score})\"\n",
    "\n",
    "# Gradio UI\n",
    "demo = gr.Interface(\n",
    "    fn=predict_news_topic,\n",
    "    inputs=gr.Textbox(lines=3, placeholder=\"Enter a news headline...\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"📰 News Topic Classifier (DistilBERT)\",\n",
    "    description=\"Fine-tuned DistilBERT model on AG News dataset. Enter a news headline and get its topic prediction.\"\n",
    ")\n",
    "\n",
    "# Launch the app\n",
    "demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OXXZooLc8eH9"
   },
   "outputs": [],
   "source": [
    "import nbformat\n",
    "from google.colab import _message\n",
    "\n",
    "# 1. Get current notebook JSON\n",
    "nb_json = _message.blocking_request('get_ipynb')['ipynb']\n",
    "\n",
    "# 2. Parse into nbformat object\n",
    "nb = nbformat.from_dict(nb_json)\n",
    "\n",
    "# 3. Remove problematic metadata at notebook level\n",
    "if \"widgets\" in nb[\"metadata\"]:\n",
    "    print(\"Removing notebook-level widgets metadata...\")\n",
    "    del nb[\"metadata\"][\"widgets\"]\n",
    "\n",
    "if \"application/vnd.jupyter.widget-state+json\" in nb[\"metadata\"]:\n",
    "    print(\"Removing notebook-level widget-state metadata...\")\n",
    "    del nb[\"metadata\"][\"application/vnd.jupyter.widget-state+json\"]\n",
    "\n",
    "# 4. Remove problematic metadata from each cell\n",
    "for cell in nb.cells:\n",
    "    if \"metadata\" in cell:\n",
    "        if \"widgets\" in cell[\"metadata\"]:\n",
    "            del cell[\"metadata\"][\"widgets\"]\n",
    "        if \"application/vnd.jupyter.widget-view+json\" in cell[\"metadata\"]:\n",
    "            del cell[\"metadata\"][\"application/vnd.jupyter.widget-view+json\"]\n",
    "\n",
    "    # Also clean outputs if present\n",
    "    if \"outputs\" in cell:\n",
    "        for out in cell[\"outputs\"]:\n",
    "            if \"data\" in out:\n",
    "                if \"application/vnd.jupyter.widget-view+json\" in out[\"data\"]:\n",
    "                    del out[\"data\"][\"application/vnd.jupyter.widget-view+json\"]\n",
    "\n",
    "# 5. Save cleaned copy\n",
    "clean_path = \"Task_1_News_Topic_Classifier_Using_BERT_2_clean.ipynb\"\n",
    "with open(clean_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    nbformat.write(nb, f)\n",
    "\n",
    "print(\"✅ Cleaned notebook saved as\", clean_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nq5nOFCy9QjV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
